# 作业51：[[Python：装饰器与偏函数]]

## 实现一个缓存装饰器

在 [[Python：函数高级#递归]] 我们实现了一个求斐波拉契数列第 $n$ 项的递归函数

```python
def fib(n:int) -> int:
    
    if  n <= 2:
        return 1
    return fib(n-1) + fib(n-2)  # 尾递归
```

这个函数存在性能问题。其原因在于已经计算过的第 $i$ 项值被丢弃了，然后后续还需要计算第 $i$ 项值。这样的重复计算带来了巨大的性能损耗

> [!tip] 
> 
> 只需要在函数递归的时候保存已经计算的值，当再次计算时直接返回已保存的值即可
> 

为了不修改原函数，这里我们实现一个缓存装饰器。为了方便后续根据参数查找已经存在的值，应该使用一个 **字典** 作为缓存容器。根据获得的参数生成一个 `key` 

```python
def make_key(args, kwargs, typed=False):
	key = args  # (1,2)
	if kwargs: # {'a':2, 'b':3}
		for item in kwargs.items():
			key += item  # key => (1,2,('a', 2), ('b', 3))
	if typed:
		key += tuple(type(item).__name__ for item in args)  # key => (1,2,('a', 2), ('b', 3), 'int', 'int')
		if kwargs:
			for value in kwargs.values():
				key += (type(value).__name__, )
	return '-'.join(map(str, key))  # 将 key 转换为字符串

def cach(func):
	results = {}
	@functools.wraps(func)
	def wrapper(*args, **kwargs):
		key = make_key(args, kwargs)
		if key in results:
			return results[key]
		results[key] = func(*args, **kwargs)
		return results[key]
	return wrapper
```

这里我们使用 `timeit` 包测试缓存是否生效

```python
@cache  
def fib(n: int) -> int:  
    if n <= 2:  
       return 1  
    return fib(n - 1) + fib(n - 2)  # 尾递归  
  
timer = timeit.Timer("fib(1000)", 'from __main__ import fib')  
execution_time = timer.timeit(number=10000000)  
print("Execution time:", execution_time, "s")  
print("Average execution time:", execution_time / 100, "s")
```

输出结果如下

```
Execution time: 3.697962516998814 s
Average execution time: 0.03697962516998814 s
```

这个缓存么有淘汰机制，会占用大量的内存。为了解决这个问题，可以引入 **淘汰算法** 将不需要的键淘汰掉，从而缓解内存占用的问题

淘汰算法重要的依据就是 [[存储器层次结构#局部性]]。程序通常倾向于 **引用邻近于其他最近引用过的数据项的数据项(空间局部性)**，或者 **最近引用过的数据项本身(时间局部性)**

目前使用广泛的淘汰算法就是 **LRU(Least Recently Used) 算法**，主要思想就是 **淘汰最久未使用的数据**。实现 LRU 需要使用 [[数据结构：散列表]]+ [[数据结构：线性表]] 两种数据结构配合起来

![[Pasted image 20250120180029.png]]

通过 `key` 如果查找到了需要的节点，就将该节点重新放在链表的尾部。当缓存新的元素时，如果缓存已满则只需要将头部的节点淘汰即可

```python
from typing import Callable
import functools

def lru_cache(maxsize=0):
    def decorating_function(func):
        wrapper = _lru_cache_wrapper(func, maxsize)
        return functools.update_wrapper(wrapper, func)
    return decorating_function

def _lru_cache_wrapper(func, maxsize) -> Callable:

    cache = {}
    cache_len = cache.__len__
    root = []  # 循环链表的根
    root[:] = [root, root, None, None]  # 头部节点：[head, tail, None, None]
    PREV, NEXT, KEY, RESULT = 0, 1, 2, 3  # 每个节点的字段名：每个节点的结构都是 [前一个节点, 下一个节点, 键, 结果]。
    full = False
    def make_cache_key(args, kwargs) -> str:
        key = args  # (1,2)
        if kwargs:  # {'a':2, 'b':3}
            for item in kwargs.items():
                key += item  # key => (1,2,('a', 2), ('b', 3))
        return '-'.join(map(str, key))  # 将 key 转换为字符串

    if maxsize == 0:  # 无缓存
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
    else: # 有缓存，采用 lru 淘汰算法
        def wrapper(*args, **kwargs):
            nonlocal full, root
            # 构建 key
            key = make_cache_key(args, kwargs)
            # 从缓存获取结果
            link = cache.get(key)
            if link is not None:  # 缓存命中
                link_prev, link_next, _key, result = link
                link_prev[NEXT] = link_next
                link_next[PREV] = link_prev
                # 拿到最后一个节点
                last = root[PREV]
                last[NEXT] = root[PREV] = link
                link[PREV] = last
                link[NEXT] = root
                return result
            # 缓存不命中
            result = func(*args, **kwargs)
            if full:  # 缓存已满
                oldRoot = root
                oldRoot[KEY] = key
                oldRoot[RESULT] = result
                root = oldRoot[NEXT]
                oldKey = root[KEY]
                oldResult = root[RESULT]
                root[KEY] = root[RESULT] = None
                # 从 cache 中移除 oldkey
                del cache[oldKey]
                cache[key] = oldRoot
            else:
                # 放在循环链表尾部
                last = root[PREV]  # []
                link = [last, root, key, result]
                last[NEXT] = root[PREV] = cache[key] = link
                # 更新变量 full
                full = (cache_len() >= maxsize)
            return result
    return wrapper
```

> [!tip] 
> 
> 在 `functools` 标准库中提供了 `lru_cache()` 的实现。标准库中的实现考虑了多线程环境下的使用
> 

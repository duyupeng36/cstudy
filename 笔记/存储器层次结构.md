# 存储器层次结构

我们在之前的学习中，我们依赖一个简单的计算机系统模型，CPU执行指令，而存储器系统为CPU存放指令和数据

> [!tip] 简单的计算机系统模型
> 
> 存储器系统是一个线性的字节数组，CPU 能够在一个常数时间内访问每个存储器位置
> 

这个简单的计算机系统模型是有效的，但是它没有办法反应现代系统的实际工作方式。在介绍存储器层次结构之前，我们先了解基本的存储技术

## 存储技术

计算机技术的成功很大程度上源自于存储技术的巨大进步。早期的计算机只有几千字节的随机访问存储器。最早的 IBM PC 甚至于没有硬盘。

$1982$ 年引入的 IBM PC-XT 有 $10\ \text{MB}$ 的磁盘。到 $2015$ 年，典型的计算机已有 $300 000$ 倍于 PC-XT 的磁盘存储，而且磁盘的容量以每两年加倍的速度增长。

### 随机访问存储器

**随机访问存储器**(Random Access Memory, RAM) 分为两类：**静态的** 和 **动态的**

> [!tip] 
> 
> 静态的RAM(SRAM) 比 动态的RAM(DRAM) 更快，但也更贵
> 
> SRAM 用来作为高速缓存存储器，既可以在 CPU 芯片上，也可以在片下
> 
> DRAM 用来作为主存储器以及图形系统的帧缓冲区
> 

一个桌面系统的 SRAM 不会超过几兆字节，但是 DRAM 却有几百或几千兆字节

#### SRAM

SRAM 将 **每个位** 存储在一个 **双稳态的(bistable)** 存储器单元里。每个单元用一个 **六晶体管** 电路来实现的

> [!tip] 六晶体管电路特性
> 
> 可以 **无限期** 地保持在 **两个** 不同 **状态** 之一 。其他任何状态都是不稳定的：对于不稳定状态，电路会迅速地转移到两个稳定状态中的一个
> 

这样为双稳态类似于倒转的钟摆，如下图所示：处于不稳定状态的钟摆会快速转移到其中一个稳态上

![[Pasted image 20241026003813.png]]

> [!tip] 
> 
> 当钟摆倾斜到最左边或最右边时，它是稳定的。从其他任何位置，钟摆都会倒向一边或另 一边
> 

由于 SRAM 存储器单元的双稳态特性，只要有电，它就会永远地保持它的值。即使有干扰来扰乱电压，当干扰消除时，电路就会恢复到稳定值

#### DRAM

DRAM 将 **每个位** 存储为对一个 **电容的充电**。电容非常小，通常只有大约 $3 \times 10^{-14}$ 法拉($30$ 微法拉)。DRAM 存储器可以制造得 **非常密集**：每个单元由一个 **电容** 和一个 **访问晶体管** 组成

> [!tip] 
> 
> DRAM 存储器单元 **对干扰非常敏感**。当电容的电压被扰乱之后，它就永远不会恢复了。暴露在光线下会导致电容电压改变。
> 

很多原因会导致漏电，使得 DRAM 单元在 $10 \sim 100$ 毫秒时间内失去电荷。幸运的是，**计算机运行的时钟周期是以 _纳秒_ 来衡量的**，所以相对而言这个保持时间是比较长的

> [!important] 
> 
> 内存系统必须 **周期性地读出**，然后 **重写** 来刷新内存每一位
> 
> 某些系统可能使用 **纠错码**，其中计算机的字会被多编码几个位，这样电路可以发现并纠正一个字中任何单个错误位
> 

DRAM 芯片中的 **存储单元** 被分成 $d$ 个 **超单元**，每个超单元都由 $w$ 个 DRAM 单元组成

> [!tip] 
> 
> 一个 $d \times w$ 的 DRAM 总共存储了 $dw$ 位信息
> 

超单元被组织成一个 $r$ 行 $c$ 列的长方形阵列，这里 $rc=d$。每个超单元有形如 `(i, j)` 的地址，这里 $i$ 表示行地址，而 $j$ 表示列地址

下图展示了一个 $16 \times 8$ 的 DRAM 芯片的组织，有 $16$ 个超单元，每个超单元有 $8$ 位，$r=c=4$

![[Pasted image 20241026163934.png]]

超单元 $(2,1)$ 中存储的信息通过 **引脚** 的外部连接器流入和流出芯片。每个引脚携带 $1$ 位的信号

上图给出了 $2$ 组引脚：$8$ 个数据引脚 和 $2$ 个地址引脚。此外，还有其他携带控制信息的引脚没有显示出来

> [!tip] $8$ 个数据引脚：将 $1$ 字节数据传入芯片或从芯片中传出
> 

> [!tip] $2$ 个地址引脚：携带 $2$ 位的行和列超单元地址

每个 DRAM 芯片被连接到某个称为 **内存控制器** 的电路，这个电路可以一次传送 $w$ 位到每个 DRAM 芯片或一次从每个 DRAM 芯片传出 $w$ 位。为了读出超单元 `(i, j)` 的内容，内存控制器将行地址 $i$ 发送到 DRAM,然后是列地址$j$。DRAM 把超单元 `(i,j)` 的内容发回给控制器作为响应

> [!example] 示例
> 
> 如果要从 $16 \times 8$  的 DRAM 中读出超单元 `(2, 1)`。内存控制器发送行地址 $2$；DRAM 将编号为 $2$ 的行的整个内容都复制到 DRAM 的 **内部行缓冲**
> 
> ![[Pasted image 20241026172905.png]]
> 
> 接着，内存控制器发送列地址 $1$；DRAM 将从内部行缓冲中复制出超单元 `(2, 1)` 中的 $8$ 位，并把它们发送到内存控制器
> 
> ![[Pasted image 20241026173201.png]]
> 

> [!important] 
> 
> 将 DRAM 组织为 **二维阵列** 而不是线性数组的原因是为了 **降低引脚数量**。例如，将 $128$ 位的 DRAM 组织为 $16$ 个超单元的线性数组，地址为 $0 \sim 15$，访问一个超单元需要 $4$ 个地址引脚
> 
> 如果将 $128$ 位的 DRAM 组织为 $4 \times 4$ 个超单元的线性数组，行地址和列地址分别为 $0 \sim 3$，只需要 $2$ 个引脚即可
> 

> [!attention] 
> 
> 二维阵列组织的缺点是必须 **分两步发送地址**，这 **增加了访问时间**
> 

DRAM 芯片封装在 **内存模块** 中，它插到主板的扩展槽上。Core i7 系统使用 $240$ 个引脚的 **双列直插内存模块**，以 $64$ 位为块传送数据到内存控制器和从内存控制器传出数据

下图展示了内存模块的基本思路：模块使用 $8$ 个 $64\ \text{Mbit}$ 的 $8\ \text{M} \times 8$ 的 DRAM 芯片，总共存储 $64\ \text{MB}$。这 $8$ 个芯片编号为 $0 \sim 7$。每个超单元存储主存的 $1 \text{byte}$，而用相应超单元地址为`(i, j)` 的 $8$ 个超单元来表示主存中 **字节地址A** 处的 $64$ 位字。DRAM0 存储第一个（低位）字节，DRAM1 存储下一个字节

![[Pasted image 20241026174426.png]]

要取出内存地址A 处的一个字，内存控制器将 A 转换成一个超单元的地址 `(i, j)`，并将它发送到内存模块，然后内存模块再将 $i$ 和 $j$ 广播到每个DRAM。作为响应，每个 DRAM 输出它的 `(i, j)` 超单元的 $8$ 位内容。模块中的电路收集这些输出，并把它们合成位一个 $64$ 位字，再返回给内存控制器

通过将多个内存模块连接到内存控制器，就能够聚合成主存。在这种情况下，当控制器收到一个地址 A 时，控制器选择包含 A 的模块 $k$, 将 A 转换成它的 `(i, j)` 的形式，并将 `(i, j)` 发送到模块 $k$

#### 访问主存

数据流通过称为 **总线** 的共享电子电路在 CPU 和 DRAM 主存之间来来回回。每次 CPU 和 DRAM 主存之间的数据传送都是通过一系列步骤完成的，这些步骤称为 **总线事务**

> [!tip] 读事务：从主存传送数据到 CPU

> [!tip] 写事务：从 CPU 传送数据到主存

**总线是一组并行的导线**，能携带 _地址_ _数据_ 和 _控制信号_。数据和地址信息可以共享同一组导线，也可以使用不同的。同时，**两个以上的设备也能共享同一总线**。控制线携带的信号会 **同步事务**，并 **标识** 出当前正在被执行的 **事务的类型**。

> [!example] 
> 
> 当前关注的事务是到主存的吗？还是到诸如磁盘控制器这样的其他 IO 设备？这个事务是读还是写？
> 
> 总线上的信息是地址还是数据项？
> 

下图展示了一个计算机系统配置信息。主要部件是 **CPU 芯片**，**IO 桥接器**，**DRAM 主存**。这些部件由一对 **总线** 连接起来，其中一条总线是 **系统总线**，另一条总线是 **内存总线**

> [!tip] IO 桥接器：负责将系统总线上的电子信号翻译成内存总线的电子信息
> 
> 同时，IO 桥接器也将系统总线和内存总线连接到 **IO 总线**。像磁盘和图形卡这样的 IO 设备共享 IO 总线
> 

![[Pasted image 20241026182316.png]]

> [!tip] 系统总线：连接 CPU 和 IO 桥接器

> [!tip] 内存总线：连接 IO 桥接器和主存

> [!example] CPU 执行加载操作： `mov A, %rax`，地址 A 的内容被加载到寄存器 `%rax` 中
> 
> CPU 芯片上称为 **总线接口** 的电路在总线上发起读事务。读事务由 $3$ 个步骤组成
> 
> 第一步：CPU 将地址 A 放到系统总线上。IO 桥将信号传递到内存总线
> 
> ![[Pasted image 20241026185850.png]]
> 
> 第二步：接下来，主存感觉到内存总线上的地址信号，从内存总线读地址，从 DRAM 取出数据字，并将数据写到内存总线。IO 桥将内存总线信号翻译成系统总线信号，然后沿着系统总线传递
> 
> ![[Pasted image 20241026185906.png]]
> 
> 第三步：CPU 感觉到系统总线上的数据，从总线上读数据，并将数据复制到寄存器 `%rax` 
>   ![[Pasted image 20241026185920.png]]
> 

> [!example] CPU 执行存储操作：`movq %rax, A`，寄存器 `%rax` 的内容被写到地址 A
> 
> CPU 同样发起写事务。写事务同样有 $3$ 个基本步骤
> 
> 第一步：CPU 将地址放到系统总线上。内存从内存总线读出地址，并等待数据到达
> 
> ![[Pasted image 20241026190752.png]]
> 
> 第二步：CPU 将 `%rax` 中的数据字复制到系统总线
> 
> ![[Pasted image 20241026190840.png]]
> 
> 第三步：存从内存总线读出数据字，并且将这些位存储到DRAM中
> 
> ![[Pasted image 20241026190917.png]]
> 

### 磁盘存储

#### 磁盘结构

**磁盘** 是由一张一张的 **盘片** 堆叠而成的。盘片的两面都涂着 **磁介质** 材料。信息是通过悬浮在盘片表面的的 **读/写磁头** 读写磁介质表面来进行读取和存储的。磁盘通常包含一个或多个这样的盘片，并封装在一个密封的容器内。

![[Pasted image 20241010162938.png]]

盘片中央由一个可以旋转的 **主轴**，也称为 **转轴**，它使得盘片以固定的 **旋转速率** 旋转

> [!tip] 
> 
> 旋转速率通常是 $5400 \sim 15000$ 转每分钟
> 

每个盘片表面是由一组称为 **磁道** (track) 的同心圆组成的。每个磁道被划分为一组 **扇区** (sector)。扇区之间由一些 **间隙** (gap) 分隔开，这些间隙中不存储数据位

![[Pasted image 20241026192139.png]]

> [!tip] 磁道：一个一个的同心圆
> 

> [!tip] 扇区：磁道被划分为相等的区域，称为扇区
> 
> 每个扇区包含 **相等数量的位**（通常是 $512$ 字节），这些数据编码在扇区上的磁性材料中
> 

> [!tip] 间隙：区分不同的扇区
> 
> 间隙存储用来标识扇区的格式化位
> 

磁盘制造商使用 **柱面** (cylinder) 来描述多个盘片驱动器的构造

> [!tip] 
> 
> **柱面** 是所有盘片表面上 **到主轴中心的距离相等的磁道的集合**
> 

#### 磁盘容量

**一个磁盘上可以记录的 _最大位数_ 称为它的最大容量**，或者简称为 **容量**。磁盘容量是由 $3$ 个因数决定：**记录密度**，**磁道密度** 和 **面密度**

> [!tip] 记录密度：位/英寸
> 
> 磁道一 英寸的段中可以放入的位数
> 

> [!tip] 磁道密度：道/英寸
> 
> 从盘片中心出发半径上一英寸的段内可以有的磁道数
> 

> [!tip] 面密度：位/平方英寸
> 
> 记录密度与磁道密度的乘积
> 

下面的公式给出了一个磁盘的容量

$$
\text{磁盘容量} = \frac{\text{字节数}}{\text{扇区}} \times \frac{\text{平均扇区数}}{\text{磁道}} \times \frac{\text{磁道数}}{\text{表面}} \times \frac{\text{表面数}}{\text{盘片}} \times \frac{\text{盘片数}}{\text{磁盘}}  
$$

> [!example] 
> 
> 假设我们有一个磁盘，有 $5$ 个盘片，每个扇区 $512$ 个字节，每个面 $20000$ 条磁道，每条磁道平均 $300$ 个扇区。那么这个磁盘的容量是
> 
> $$
> \begin{aligned} 
> \text{磁盘容量} &= \frac{\text{512 字节}}{\text{扇区}} \times \frac{\text{300 扇区}}{\text{磁道}} \times \frac{\text{20000 磁道}}{\text{表面}} \times \frac{\text{2 表面}}{\text{盘片}} \times \frac{\text{5 盘片}}{\text{磁盘}}\\
> &= 512 \times 300 \times 20000 \times 2 \times 5 \text{字节}\\
> &= 30720000000 \text{字节}\\
> &=30.72 \text{GB}
> \end{aligned}  
> $$
>

> [!tip] 
> 
> 制造商采用的千兆字节不是系统采用的单位。这里 $1\ GB = 10^9\ bytes$
> 

#### 磁盘操作

磁盘用 **读/写头** 来读写存储在 **磁性表面的位**，而读写头连接到一个 **传动臂** 一端。通过沿着半径轴前后 **移动这个传动臂**，驱动器可以 **将读/写头定位** 在盘面上的 **任何磁道** 上。

> [!tip] 寻道：将读写头移到盘面的任何磁道上
> 

一旦 读/写头 定位到了期望的磁道上，那么当磁道上的每个位通过 读/写头 的下面时，读/写头可以感知到这个位的值（读该位），也可以修改这个位的值（写该位）

> [!tip] 
> 
> 有多个盘片的磁盘针对 **每个盘面都有一个独立的读/写头**
> 

![[Pasted image 20241026202414.png]]

在传动臂末端的 读/写头 在磁盘表面高度大约 $0.1$ 微米处的一层薄薄的气垫上飞翔

> [!tip] 
> 
> 读/写头是悬浮在盘面上方 $0.1$ 微米处，而不是与盘面接触的
> 

在这样小的间隙里，盘面上一粒微小的灰尘都像一块巨石。如果 读/写头 碰到了这样的一块巨石，读/写头会停下来，撞到盘面一所谓的 **读/写头冲撞**

磁盘以扇区大小的 **块** 来读写数据。对扇区的访问时间(有三个主要的部分：**寻道时间**，**旋转时间** 和 **传送时间**

> [!tip] 寻道时间：传动臂将读写头定位到目标磁道的时间
> 
> 移动传动臂所需的时间称为 **寻道时间**。寻道时间依赖两个因素：**读写头当前位置** 和 **传动臂移动速度**
> 
> 现代驱动器中的平均寻道时间通常是 $3 \sim 9$ 毫秒；一次寻道的最大时间可达 $20$ 毫秒
> 

> [!tip] 旋转时间：等待目标扇区旋转到 读/写头 下方
> 
> 旋转时间也依赖两个因素：**寻道成功时盘面的位置** 和 **磁盘的旋转速度**
> 
> 在最坏的情况下，读/写头刚刚错过了目标扇区，必须等待磁盘转一整圈。因此，**最大旋转延迟**（以秒为单位）是
> 
> $$
> \frac{1}{\text{RPM}} \times \frac{60\ s}{1\ min}
> $$
> 
> 平均旋转延迟是最大旋转延迟的一半
> 

> [!tip] 传送时间：驱动器读写扇区的时间
> 
> 一个扇区的传送时间依赖于 **旋转速度** 和 **每条磁道的扇区数目**
> 
> 平均传送时间如下
> 
> $$
> \frac{1}{\text{RPM}} \times \frac{1}{平均扇区数/磁道} \times \frac{60\ s}{1\ min}
> $$
> 

#### 连接 IO 设备

例如图形卡、监视器、鼠标、键盘和磁盘这样的 IO 设备，都是通过 **IO总线**

> [!example] 
> 
> Intel 的外围设备互连(Peripheral Component Interconnect , PCI)总线连接到 CPU 和 主存的
> 

系统总线和内存总线是与 CPU 相关的。然而，诸如 PCI 这样的 IO 总线设计成与底层 CPU 无关

在 [[计算机组成：计算机组成原理#子系统的互联#IO设备的连接]] 中介绍了几种 IO 总线。这里不在赘述。下图给出了总线结构的示例

![[Pasted image 20241026205704.png]]

#### 访问磁盘

关于 IO 设备是如何工作的我们不做详细论述。CPU 使用一种称为 [[内存映射]] I/O 的技术向 IO 设备发射命令

![[Pasted image 20241026210739.png]]

在使用内存映射 IO 的系统中，地址空间中有一块地址是为与 IO 设备通信保留的，每个这样的地址称为一个 **IO 端口**

> [!tip] 
> 
> 当一个设备连接到总线时，它与一个或多个端口相关联，或它被映射到一个或多个端口
> 

来看一个简单的例子：假设磁盘控制器映射到端口 `0XA0`。随后，CPU 可能通过执行 $3$ 个对地址 `0XA0` 的存储指令，发起磁盘读
+ 第一条指令是发送一个命令字，告诉磁盘发起一个读，同时还发送了其他的参数
+ 第二条指令指明应该读取的逻辑块号
+ 第三条指令指明应该存储磁盘扇区的内容的主存地址

> [!attention] 当 CPU 发出了请求指令后，**在磁盘执行读取指令的时候，它通常会做一些其他的工作**
> 
> 回想一下，一个 $1\ GHz$ 的处理器时钟周期为 $1\ ns$，在用读磁盘的 $16\ ms$ 时间里，它潜在的可能执行 $1600$ 万条指令。**在传输进行时，只是简单的等待生么也不做，是一种极大的浪费**
> 

在传输进行时，只是简单地等待，什么都不做，是一种极大的浪费。

> [!tip] DMA
> 
> 在磁盘控制器收到来自 CPU 的读指令之后，它将逻辑号翻译成一个扇区地址，读该扇区的内容，然后将这些内容直接传送到主存，不需要 CPU干涉
> 
> 设备可以自己执行读或写总线事务不需要 CPU 干涉的过程，称为 **直接内存访问(DMA)**。这种数据传送称为 **DMA 传送**
> 
> ![[Pasted image 20241026222230.png]]
> 
> 
> 

在 DMA 传送完成，磁盘扇区的内容被安全的存储在主存中以后，**磁盘控制器通过给 CPU 发送一个 _中断信号_ 来通知 CPU**。基本思想是中断会发信号到 CPU 芯片的一个外部引脚上。这会 **导致 CPU 暂停它当前的正在做的工作**，跳转到一个操作系统例程。这个程序会记录下 IO已完成，然后将控制返回到 CPU 被中断的地方

![[Pasted image 20241026222403.png]]

## 局部性

一个编写良好的计算机程序常常具有良好的 **局部性**，也就是，它们倾向于 **引用邻近于其他最近引用过的数据项的数据项**，或者 **最近引用过的数据项本身**。这种倾向性被称为 **局部性原理**，是一个持久的概念，对硬件和软件的设计和性能都有着极大的影响

局部性通常有两种不同的形式：**时间局部性** 和 **空间局部性**

> [!tip] 时间局部性
> 
> 在一个具有良好 **时间局部性** 的程序中，**被引用过一次的内存位置很有可能在不远的将来再被多次引用**
> 

> [!tip] 空间局部性
> 
> 在一个具有良好 **空间局部性** 的程序中，如果 **一个内存位置被引用了一次**，那么程序很可能 **在不远的将来 _引用附近_ 的一个内存位置**
> 

一般而言，**具有良好局部性的程序比局部性差的程序运行的更快**。现代计算机系统的各个层次，从硬件到操作系统，再到应用程序，它们的设计都利用了局部性

> [!tip] 
> 
> **在硬件层**，局部性原理运行计算机设计者通过引入称为 **高速缓存存储器** 的小而快速的存储器来 **保存最近被引用的指令和数据项，从而提高主存的访问速度**
> 
> **在操作系统级**，局部性原理允许系统 **使用主存作为 _虚拟内存_ 最近被引用的块高速缓存**。类似的，操作系统用 **主存缓存磁盘文件系统** 中最近被使用的 **磁盘块**
> 
> **在应用程序级**，局部性原理也非常重要，例如，Web 浏览器将最近被引用的文档放在本地磁盘上，利用的就是时间局部性。大容量的 Web服务器将最近被请求的文档放在前端磁盘高速缓存中，这些缓存能满足最这些文档的要求，而不需要服务器任何干预
> 

###  对程序数据引用的局部性

如下代码中，它对一个向量的元素求和

```c
int sumvec(int v[N]) {
    int i, sum = 0;
    for(i = 0; i < N; i++) {
        sum += v[i];
    }
    return sum;
}
```

这个例子中，变量 `sum` 在每次循环迭代中被引用一次，因此，对于 `sum` 来说，有好的时间局部性。另一方面，因为 `sum` 是标量，对于 `sum` 来说，没有空间局部性

向量 `v` 的元素是被顺序读取的，一个接一个，按照它们存储在内存中的顺序，因此，对于变量 `v`，函数有很好的空间局部性，但是时间局部性很差，因为每个向量元素只被访问一次

![[Pasted image 20241026224635.png]]

对于循环体中的每个变量，这个函数要么有好的空间局部性，要么有好的时间局部性，**所以 `sumvec` 函数有良好的局部性**

对于上述例子，顺序访问一个向量中的每个元素的函数，具有 **步长为 $1$ 的引用模式**，称为 **顺序引用模式**。一个连续向量中，每隔 $k$ 个元素进行访问，就称为 **步长为 $k$ 的引用模式**。步长为 $1$ 的引用模式是程序中空间局部性常见的重要来源

> [!tip] 
> 
> 一般而言，随着步长的增加，空间局部性下降
> 

如下例子是引用多维数组的程序，它对一个二维数组的元素求和。双重嵌套循环按照行优先顺序读数组元素。也就是，内层循环读第一行的元素，然后读第二行的元素

```c
int sumarrayrows(int a[M][N]) {
    int i, j, sum = 0;
    for(i = 0; i < M; i++) {
        for(j = 0; j < N; j++) {
            sum += a[i][j];
        }
    }
    return sum;
}
```

**函数 `sumarrayrows` 具有良好的空间局部性**，因为他 **按照数组被存储的 _行优先顺序_ 来访问**这个数组

![[Pasted image 20241026224933.png]]

一个微小的改动，能够对它的局部性有很大的影响。函数 `sumarraycols` 的局部性很差，因为它 **按照列顺序来扫描数组**，而不是按照行顺序。因为 C 数组在内存中按照行顺序存放，结果就得到步长为 $N$ 的引用模式

```c
int sumarraycols(int a[M][N]) {
    int i, j, sum = 0;
    for(j = 0; j < N; j++) {
        for(i = 0; i < M; i++) {
            sum += a[i][j];
        }
    }
    return sum;
}
```

![[Pasted image 20241026225053.png]]

### 取指令局部性

因为程序指令在存放在内存中，CPU 必须取出这些指令，所以我们能够评价程序关于取指令的局部性。函数 `sumvec` 中 for 循环体里的指令是按照连续的内存顺序执行的，因此循环有良好的空间局部。因为循环体会被执行多次，所以也有良好的时间局部性

代码区别于程序数据的一个重要属性是在运行时它不能被修改。当程序正在执行时，CPU 只能从内存中读出它的指令。CPU 很少会重写或修改这些指令

## 存储器层次结构

> [!tip] 存储技术
> 
> 不同的存储技术的访问时间差异很大。速度较快的技术每个字节的成本要比速度慢的技术高，而且容量较小。CPU 和主存之间的速度差距在增大
> 

> [!tip] 局部性
> 
> 一个编写良好的程序倾向于展示出良好的局部性
> 

计算中一个喜人的巧合是，硬件和软件的这些属性互补得很完美。它们这种互相补充得性质使人想到一种组织存储器系统得方法，称为 **存储器层次结构**，所有的现代计算机系统中都使用了这种方法

下图展示了一个典型得存储器层次结构。一般而言，从高层向底层走，存储设备变得更慢更便宜也更大

![[Pasted image 20241026230327.png]]

> [!tip] 
> 
> 在最高层，是少量快速得 CPU 寄存器，CPU 可以在一个 CPU 时钟周期内访问它们
> 
> 接下来一层是一个或多个小型到中型的基于 **SRAM** 的高速缓存存储器，可以 **在几个 CPU 时钟周期内访间它们**
> 
> 然后是一个大的基于 **DRAM** 的主存，可以在 **几十到几百个时钟周期内访问它们**
> 
> 接下来是 **慢速** 但是容扯很大的 **本地磁盘**
> 
>最后，有些系统甚至包括了一层附加的 **远程服务器上的磁盘**，要通过 **网络** 来访问它们
>

### 存储器层次结构中的缓存

**高速缓存** 是一个小而快速的存储设备，**作为存储在更大、也更慢的设备中的数据对象的缓冲区域**

> [!tip] 
> 
> 存储器层次结构的中心思想是：**对于每个 $k$， 位于 $k$ 层的 _更快更小_ 的存储设备作为位于 $k+1$ 层的 _更慢更大_ 的存储的 _缓存_**。换句话说，层次结构中的每一层都缓存来自较低一层的数据对象
> 
> 例如，本地磁盘作为通过网络从远程磁盘取出文件的缓存；主存作为本地磁盘上数据的缓存
> 

下图展示了存储器层次结构中缓存的一般性概念。第 $k + 1$ 层的存储器被划分成连续的 **数据块**。每个块都有一个唯一的地址或名字，使之区别于其他块

![[Pasted image 20241026231351.png]]

> [!tip] 块可以是固定大小，也可以是可变大小的
> 
> 块大小通常是固定的。然而，本地磁盘作为远程磁盘的缓存时，块可以不是固定的；例如，Web 服务器上的远程 HTML 文件
> 

类似地，**第 $k$ 层的存储器被划分成较少的块的集合**，每个块的大小与 $k+1$ 层的块的大小一样。在任何时刻，第 $k$ 层的缓存包含第 $k+1$ 层块的一个子集的副本。例如，在上图中，第 $k$ 层的缓存有 $4$ 个块的空间，当前包含块 $4, 9, 14$ 和 $3$ 的副本

**数据总是以块大小为传送单元在第 $k$ 层和第 $k+1$ 层之间来回复制的**。虽然在层次结构中 任何一对相邻的层次之间块大小是固定的，但是其他的层次对之间可以有不同的块大小

> [!tip] 
> 
> 一般而言，层次结构中较低层 (离CPU较远) 的设备的访问时间较长，因此为了补偿这些较长的访问时间，倾向于使用较大的块
> 

#### 缓存命中

当程序需要第 $k + 1$ 层的某个数据对象 $d$ 时，它首先在当前存储在第 $k$ 层的一个块中查找 $d$。如果 $d$ 刚好缓存在第 $k$ 层中，那么就是我们所说的 **缓存命中**

程序直接从第 $k$ 层读取 $d$，根据存储器层次结构的性质，这要比从第 $k + 1$ 层读取 $d$ 更快

#### 缓存不命中

如果第 $k$ 层中没有缓存数据对象 $d$，那么就是我们所说的 **缓存不命中**。当发生缓存不命中时，第 $k$ 层的缓存从第 $k+1$ 层缓存中取出包含 $d$ 的那个块，如果第 $k$ 层的缓存已经满了，可能就会 **覆盖** 现存的一个块

> [!tip] 覆盖
> 
> 覆盖一个现存的块的过程称为 **替换** 或者 **驱逐** 这个块。被驱逐的块有时也称为 **牺牲块**。决定替换哪个块是由缓存的 **替换策略** 来控制
> 
> +  **随机替换策略** 的缓存会随机选择一个牺牲块
> +  **最近最少被使用(LRU)** 替换策略的缓存会选择那个最后被访问的时间距离现在最远的块
> 

在第 $k$ 层缓存从第 $k + 1$ 层缓存取出那个块之后，程序就能像前面一样从第 $k$ 层读出 $d$ 了

#### 缓存不命中的种类

如果第 $k$ 层的缓存是空的，那么对任何数据对象的访问都会不命中。一个空的缓存有时被称为 **冷缓存**，此类不命中称为 **强制性不命中** 或 **冷不命中**

> [!tip] 
> 
> 冷不命中很很重要，因为它们通常是 **短暂的事件**，不会在反复访问存储器使得缓存暖身之后的稳定状态中出现
> 

只要发生了不命中，第 $k$ 层的缓存就必须执行某个 **放置策略**，确定把它从第 $k + 1$ 层中取出的块放在哪里

> [!tip] 
> 
> 最灵活的替换策略是允许来自第 $k + 1$ 层的任何块在第 $k$ 层的任何块中
> 
> 对于存储层次结构中 **高层的缓存**，它们是 **用硬件来实现的**，而且速度是最优的，这个策略实现起来通常很贵的，**因为随机的放置块，定位起来代价很高**
> 

因此，硬件缓存通常使用的是 **更严格的放置策略**：**这个策略将第 $k + 1$ 层的某个块 _限制_ 放置在第 $k$ 层块的一个小的子集中**。下图中，我们可以确定 **第 $k + 1$ 层的块 $i$ 必须放置在第 $k$ 层的块 $(i\ \%\ 4)$ 中**

![[Pasted image 20241026231351.png]]

> [!example] 
> 
> 例如，第 $k + 1$ 层的块 $0,  4, 8, 12$ 会映射到第 $k$ 层的块 $0$；第 $k + 1$ 层的块 $1, 5, 9, 13$ 会映射到第 $k$ 层的块 $1$
> 

> [!tip] 冲突不命中
> 
> 这种限制性的放置策略会引起一种不命中，称为 **冲突不命中**。这种情况中，缓存足够大，能够保存被引用的数据对象，但是因为这些对象会映射到同一个缓存块，**缓存会一直不命中**
> 
> 如果程序请求块 $0$，然后块 $8$，然后块 $0$，然后块 $8$，以此类推，在第 $k$ 层的缓存中，对这两个块的每次引用都会不命中，即使这个缓存总共可以容纳 $4$ 个块
> 

**程序通常是按照 _一系列阶段_ 来运行的，每个阶段 _访问缓存块的某个相对稳定不变_ 的集合**。例如，一个嵌套的循环可能会反复地访问同一个数组元素。这个块的集合称为这个阶段的 **工作集**

> [!tip] 容量不命中
> 
> 当工作集的大小超过缓存的大小时，缓存会经历 **容量不命中**。换句话说就是，不能处理这个工作集
> 

#### 缓存管理

存储器层次结构的本质是，**每一层存储设备都是较低一层的缓存**。在每一层上，某种形式的逻辑必须 **管理缓存**

> [!tip] 管理缓存
> 
> 这里，我们的意思是指 **某个东西要 _将缓存划分成块_，_不同的层之间传送快_，判定是命中还是不命中，并处理它们**。管理缓存的逻辑可以是硬件、软件，或者两者结合
> 

## 高速缓存存储器

早期计算机系统的存储层次结构只有三层：CPU 寄存器、DRAM 主存储器 和 磁盘存储。不过，由于 CPU 和主存之间逐渐增大的差距，系统设计者被迫在 CPU 寄存器文件和主存之间插入了一个小的 **SRAM 高速缓存存储器**，称为 **L1 高速缓存**。如下图，**L1 高速缓存的访问速度几乎和寄存器一样快，典型地大约 $4$ 个时钟周期**

![[Pasted image 20241027103901.png]]

随着 CPU 和主存之间的性能差距不断增大，系统设计者在 L1 高速缓存和主存之间又插入了一个更大的高速缓存，称为 **L2 高速缓存**，可以大约 $10$ 个时钟周期内访问到它。有些现代系统还包括有一个更大的高速缓存，称为 **L3 高速缓存**，在存储器层次结构中，它位于 L2 高速缓存和主存之间，可以在大约 $50$ 周期内访问到它

出于理论研究，我们假设一个简单的存储器层次结构，**CPU 和主存之间只有一个 L1 高速缓存**

### 通用的高速缓存存储器组织结构

考虑一个计算机系统，其中每个存储器地址有 $m$ 位，形成 $M = 2^m$ 个不同的地址。如下图所示，这样一个机器的高速缓存被组织成一个有 $S = 2^s$ 个 **高速缓存组的数组**。每个组包含 $E$ 个 **高速缓存行**。每行是由一个 $B = 2^b$ 字节的 **数据块** 组成的，一个 **有效位** 指明这个行是否包含有意义的信息，还有 $t = m - (b + s)$ 个 **标记位**

![[Pasted image 20241027112108.png]]

> [!tip] 
> 
> 有效位和标记位标识该行缓存的数据是否有效。
> 
> 高速缓存的结构可以用元组 `(S 组, E 行, B 块, m 位)` 来描述。高速缓存的大小 $C$ 指的是所有块的大小的和，其中不包含标记位和有效位
> 
> $$
> C = S \times E \times B
> $$
> 

当一条加载指令指示 CPU 从主存地址 A 读一个字时，它将地址 A 发送到高速缓存。如果高速缓存正保存着地址 $A$ 处地那个字地副本，它就立即将那个字发回给 CPU

**那么高速缓存如何知道它是否包含 _地址 A_ 处那个字地副本呢？**。高速缓存地结构使得它能够简单地检查地址位，找到所请求地字，类似于使用极其简单地哈希函数地哈希表。

> [!tip] 地址结构：高速缓存的参数 $S$ 和 $B$ 将地址分为了 $3$ 个字段，如下图
> 
> ![[Pasted image 20241027112724.png]]
> 
> 地址 $A$ 中的 $s$ 个 **组索引位** 可以确定一个到 $S = 2^s$ 个组的索引。第一个组为的索引位 $0$。**组索引位被解释为一个无符号整数**，它告诉我们这个字必须存储在哪个组中
>  
> 一旦我们知道了这个字必须存放在哪个组中，地址 A 中的 **$t$ 个标记位** 就告诉我们 **这个组在哪一行包含这个字**
> 
> 一旦我们由组索引标识的组中定位了由标号所标识的行，那么 地址的 **$b$ 个块偏移位** 给出在 $B$  个字节的数据块中的 **字偏移**
> 

高速缓存的描述使用了很多符号，这些符号的含义如下表

| 参数                       | 描述                 |
| :----------------------- | ------------------ |
| 基本参数                     |                    |
| $S=2^s$                  | 组数                 |
| $E$                      | 行数                 |
| $B=2^b$                  | 块大小                |
| $m = \log_2(M)$          | 主存物理地址位数           |
|                          |                    |
| 衍生参数                     |                    |
| $M=2^m$                  | 内存地址的最大数量          |
| $s = \log_2(S)$          | 组索引位数              |
| $b=\log_2(B)$            | 块偏移的位数             |
| $t = m-(s+b)$            | 标记位数               |
| $C = B\times E \times S$ | 不包括有效位额标记位的高速缓存的大小 |

### 直接映射高速缓存

根据每个组的高速缓存行数 $E$，高速缓存被分为不同的类。每个组只有一行($E=1$) 的高速缓存称为 **直接映射** 高速缓存。它是最容易实现和理解的

![[Pasted image 20241027211124.png]]

> [!example] 
> 假设我们有这样一个系统，它有一个CPU、一个寄存器文件、一个 L1 高速缓存和一个主存。当CPU 执行一条读内存字 $w$ 的指令，它向 L1 高速缓存请求这个字
> 
> 如果 L1 高速缓存有 $w$ 的一个缓存的副本，那么就得到 L1 高速缓存命中，高速缓存会很快抽取出 $w$，并将它返回给CPU
> 
> 否则就是缓存不命中，当 L1 高速缓存向主存请求包含 $w$ 的块的一个副本时，CPU 必须等待。当被请求的块最终从内存到达时，L1 高速缓存将这个块存；放在它的一个高速缓存行里，从被存储的块中抽取出字 $w$，然后将它返回给 CPU
> 

高速缓存确定一个请求是否命中，然后抽取出被请求的字的过程分为 $3$ 步：**组选择**，**行匹配** 和 **字抽取**

#### 组选择

高速缓存从 $w$ 位的地址中间抽取出 $s$ 个组索引位。这些位被解释成一个对应于一个组号的无符号整数

> [!tip] 
> 
> 如果我们 **把高速缓存看成是一个关于组的一维数组**，那么这些组索引位就是一个到这个数组的索引
> 

下图展示了直接映射高速缓存的组选择是如何工作的

![[Pasted image 20241027211959.png]]

> [!example] 
> 
> 组索引位模式 $00001_2$ 被解释为一个选择组 $1$ 的整数索引
> 

#### 行匹配

上一步已经选中了某个组 $i$，下面就是要确定字 $w$ 的一个副本是否存储在组 $i$ 包含的一个高速缓存行中

> [!tip] 
> 
> 在直接映射高速缓存中非常容易，而且很快，这是因为每个组只有一行。当且仅当 **设置了有效位**，而且 **高速缓存 _行中的标记_ 与 $w$ 的 _地址中的标记_ 相匹配** 时，这一行中包含 $w$ 的一个副本
> 

下图展示了直接映射高速缓存中的行匹配是如何工作的

![[Pasted image 20241027212549.png]]

> [!example] 
> 
> 选中的组 $i$ 中只有 $1$ 个高速缓存行，这个 **行的有效位设置了**，所以我们知道 **标记** 和 **块中的位** 是 **有意义** 的
> 
> 因为这个 **高速缓存 _行中的标记位_ 与 _地址中的标记位_ 相匹配**，所以我们知道我们想要的那个字的一个副本确实存储在这个行中
> 
> 经过上面两个步骤，我们就得到缓存命中。也就是，如果 **有效位没有设置** 或者 **标记不匹配**，那么我们就得到缓存不命中
> 

#### 字选择

一旦命中，我们知道 $w$ 就在这个块中的某个地方。最后一步确定所需要的字在块中是从哪里开始的。如下图所示，**块偏移位提供了所需要的字的第一个字节的偏移**。

![[Pasted image 20241027212549.png]]

>[!tip] 
>
>像我们把高速缓存看成一个行的数组一样，我们 **把块看成一个字节的数组**，而 **字节偏移** 是到这个数组的一个索引
>

> [!example] 
> 
> 本例中，块偏移位是 $100_2$ 它表明 $w$ 的副本是从块中的字节 $4$ 开始的（我们假设字长为 $4$ 字节）
> 

#### 不命中时的行替换

如果缓存不命中，那么它需要从存储器的层次结构中的下一层取出被请求的块，然后将新的块存储在组索引为指示的组中的一个高速缓存行中

> [!tip] 
> 
> 一般而言，**如果组中都是有效高速缓存行了，那么就必须驱逐一个现存行**
> 
> 直接映射高速缓存每个组只包含一行，用新取出的行替换当前行即可
> 

#### 运行中的直接映射高速缓存

高速缓存用来选择组和标识行的机制极其简单，因为硬件必须在几个纳秒的时间内完成这些工作。不过，用这种方式来处理位是很令人困惑的。一个具体的例子能帮助解释清楚这个过程

> [!example] 
> 
> 假设我们有一个直接映射高速缓存，描述为：$(S, E,B, m) = (4, 1, 2, 4)$。如下所示
> 
> ![[Pasted image 20241027224525.png]]
> 
> 
> 也就是说，高速缓存有 $4$ 个组，每个组有 $1$ 行，每个块 $2$ 个字节，地址是 $4$ 位的。还假设每个字是单字节的
> 

对于上面示例中的直接映射缓存，其地址如下图所示

![[Pasted image 20241027221926.png]]

初学高速缓存时，列举出整个地址空间并划分好位是很有帮助的。下图列举出了整个空间

![[Pasted image 20241027215257.png]]

下面我们模拟 CPU 执行一系列读的时候，高速缓存的执行情况。我们假设 CPU 读 $1$ 字节

>[!example] **初始时**
>
>高速缓存是空的，即每个有效位都是 $0$
>
>![[Pasted image 20241027220039.png]]
>
>每一行都代表了高速缓存行。第一列表明该行所属的组，记住提供这个位只是为了方便，实际上它并不真是高速缓存的一部分；后面四列代码表每个高速缓存行的实际的位
>

> [!example] **读地址 0 的字**
> 
> 因为组 $0$ 的有效位是 $0$，是缓存不命中。高速缓存从内存取出块 $0$，并这个块存储在组 $0$ 中。然后，高速缓存返回新取出的高速缓存行 `块[0]` 的 `m[0]`，内存位置 $0$ 的内容
> 
> ![[Pasted image 20241027220459.png]]
> 

> [!example] **读地址 $1$ 的字**
> 
> 这次会是高速缓存命中。高速缓存立即从高速缓存行的 `块[1]` 中返回 `m[1]`。高速缓存的状态没有变化
> 


> [!example] **读地址 $13$ 的字**
> 
> 由于组 $2$ 中的高速缓存行不是有效的，所以缓存不命中。高速缓存把 `块[6]` 加载到组 $2$ 中，然后从新的高速缓存行的 `块[1]` 中返回 `m[13]`
> 
> ![[Pasted image 20241027222059.png]]
> 

> [!example] **读地址 8 的字**
> 
> 这会发生缓存不命中。组 $0$ 中的高速缓存行确实有效，但实际标记不匹配。高速缓存将块 $4$ 加载到组 $0$ 中（替换地址 $0$ 时读入的那一行），然后从新的高速缓存行的 `块[0]` 中返回 `m[8]`
> 
> ![[Pasted image 20241027222329.png]]
> 

> [!example] **读地址 0 的字**
> 
> 又会发生缓存不命中，因为前面引用地址 8 时，我们刚好替换了块 0。这就是 **冲突不命** 中的一个例子，也就是我们有足够的缓存空间，但是却交替引用映射到同一个组的块
> 
> ![[Pasted image 20241027222409.png]]
> 

#### 冲突不命中

冲突不命中在真实的程序中很常见，这会导致令人困惑的性能问题。当程序访问大小为 $2$ 的幂的数组时，直接映射高速缓存中通常会发生冲突不命中。例如考虑一个计算两个向量点积的函数

```c
float dotprod(float x[8], float y[8]) {
    float sum = 0.0;
    int i;
    for(i = 0; i < 8; i++) {
        sum += x[i] * y[i];
    }
    return sum;
}
```

对于 `x` 和 `y` 来说，这个函数有良好的空间局部性，因此我们期望它的命中率会比较高。不幸的是，并不总是如此。下面情形中，`x` 和 `y` 被映射到同一个组

> [!tip] 
> 
> **假设浮点数是 $4$ 个字节**，`x` 被加载到从地址 $0$ 开始的 $32$ 字节连续内存中，而 `y` 紧跟在 `x` 之后，地址从 $32$ 开始
> 
> 为了简便，**假设一个块是 $16$ 个字节**，足够容纳 $4$ 个浮点数，**高速缓存由 $2$ 个组组成**，**高速缓存的大小为 $32$ 字节**。我们假设变量 `sum` 实际上存放在一个 CPU 寄存器中，因此不需要内存引用。根据这些假设每个 `x[i]` 和 `y[j]` 会映射到相同的高速缓存组
> 
> ![[Pasted image 20241027223507.png]]
> 

在运行时，循环的第一次迭代引用 `x[0]`，缓存不命中会导致包含 `x[0] ~ x[3]` 的块被加载到组 $0$。接下来是对 `y[0]` 的引用，又一次缓存不命中，导致包含 `y[0] ~ y[3]` 的块被复制到组 $0$，覆盖前一次引用复制进来的 `x` 的值

下一次迭代中，对 `x[1]` 的引用不命中，导致 `x[0] ~ x[3]` 的块被加载回组 $0$，覆盖掉 `y[0] ~ y[3]` 的块。因而现在我们就有了一个 **冲突不命中**

> [!important] 抖动：高速缓存 _反复_ 地 _加载_ 和 _驱逐_ **相同** 的高速缓存块的 _组_
> 
> 而实际上后面每次对 `x` 和 `y` 的引用都会导致冲突不命中，因为我们在 `x` 和 `y` 的块之间 **抖动**：高速缓存反复地加载和驱逐相同地高速缓存块的组
> 

也就是说，即使程序有良好地空间局部性，而且我们地高速缓存中也有足够地空间存放 `x[i]` 和 `y[j]` 的块，每次引用还是会导致冲突不命中，这是 **因为这些块被映射到了同一个高速缓存组**

幸运的是，一旦程序员意识到了发生什么，就很容易 **修正抖动问题**。一个很简单的方法是 **在每个数组的结尾放 $B$ 字节的填充**。例如，不是将 `x` 定义为 `float x[8]`，而是定义成 `float x[12]`。假设在内存中 `y` 紧跟在 `x` 的后面，我们有下面这样的数组元素到组的映射

![[Pasted image 20241027225355.png]]

> [!tip] 
> 
> 在 `x` 结尾加了填充，`x[i]` 和 `y[i]` 现在就映射到了不同的组，消除了抖动冲突不命中
> 


> [!question] 你也许会奇怪，为什么高速缓存用中间的位来作为组索引，而不是用高位
> 
> 用中间的位更好，是有很好的原因的。下图说明了原因
> 
> ![[Pasted image 20241028112236.png]]
> 
> 如果高位用做索引，那么一些连续的内存块就会映射到相同的高速缓存块
> 
> 例如，在上图中，头四个块映射到笫一个高速缓存组，笫二个四个块映射到笫二个组，依此类推。如果一个程序有良好的空间局部性，顺序扫描一个数组的元素，那么在任何时刻，**高速缓存都只保存着一个块大小的数组内容**
> 
> 这样对高速缓存的使用效率很低。相比较而言，**以中间位作为索引，相邻的块总是映射到不同的高速缓存行**
> 

### 组相联高速缓存

**直接映射高速缓存中冲突不命中造成的问题** 源于每个组只有一行( $E=1$ )这个限制。我们放宽这条限制，一个 $1 \lt E \lt C/B$ 的高速缓存通常称为 $E$ 路组相联高速缓存。下图展示了一个 $2$ 路组相联高速缓存的结构

![[Pasted image 20241028112522.png]]

#### 组选择

它的组选择与直接映射高速缓存的组选择一样。如下图总结了这个原理

![[Pasted image 20241028112638.png]]

#### 行匹配和字选择

它必须检查 **多个行** 的 **标记位** 和 **有效位**，以确定所请求的字是否在集合中。传统的内存是一个值的数组，以地址作为输入，并返回存储在那个地址的值。相联存储器是一个 `(key, value)` 对的数组，以 `key` 作为输入，返回与 `key` 相匹配的 `(key, value)` 中的 `value` 值

> [!tip] 
> 
> 因此，我们可以把组相联高速缓存中的每个组都看成一个小的相联存储器，`key` 是 **标记** 和 **有效位** ，而 `value` 就是 **块的内容**
> 

| key            | value |
| :------------- | :---- |
|  _标记位_ 和 _有效位_ | 块的内容  |

> [!tip] 行匹配的基本思想
> 
> 相联高速缓存中 **行匹配** 的基本思想：这里的一个重要思想就是组中的任何一行都可以包含任何映射到这个组的内存块。所以高速缓存必须搜索组中的每一行，**寻找一个 _有效的行_**，**_行的标记_ 与 _地址中的标记_ 相匹配**
> 
> ![[Pasted image 20241028113157.png]]
> 

> [!tip] 字选择
> 
> 如果高速缓存找到了这样一行，那么我们就的到缓存命中，**块偏移从这个块中选择一个字**，和直接映射高速缓存一样
> 

#### 行替换

如果 CPU 请求的字不在组的任何一行中，那么就是缓存不命中，高速缓存必须从内存中取出包含这个字的块。不过，一旦高速缓存取出了这个块，该替换哪个行呢? **当然，如果有一个空行，那它就是个很好的候选**。但是 如果该组中没有空行，那么我们必须从中选择一个非空的行，**希望 CPU 不会很快引用这个被替换的行**。

> [!tip] 行替换策略
> 行替换策略有两种：**替换空行** 和 **替换 CPU 不会很快引用的行** 
> 
> 替换 CPU 不会很快引用的行使用了局部性原理
> 
> + **最不常使用(Least-Frequently-Used, LFU)策略** 会替换在过去某个时间窗口内引用次数最少的那一行
>
> + **最近最少使用(Least-Recently-Used, LRU)策略** 会替换最后一次访问时间最久远的那一行
> 
> 

所有这些策略都需要额外的时间和硬件。但是，越往存储器层次结构下面走，远离 CPU, 一次不命中的开销就会更加昂贵，用更好的替换策略使得不命中最少也变得更加值得了

### 全相联高速缓存

全相联高速缓存 是由一个 **包含所有缓存行的组**（即 $E = C/B$）组成。下图显示了它的基本结构

![[Pasted image 20241028113730.png]]

#### 组选择

全相联高速缓存中的组选择非常简单，因为只有一个组。**地址中没有组索引位**，地址只被划分成了一个标记和一个块偏移。

![[Pasted image 20241028113851.png]]

#### 行匹配和字选择

全相联高速缓存中的行匹配和字选择与组相联高速缓存中的是一样的，它们之间的区别主要是规模大小的问题。

![[Pasted image 20241028114005.png]]

因为高速缓存电路必须 **并行地搜索许多相匹配的标记**，构造一个又大又快的相联高速缓存很困难，而且很昂贵。因此，**全相联高速缓存只适合做小的高速缓存**，例如虚拟内存系统中的翻译 **备用缓冲器(TLB)**，它缓存页表项

### 高速缓存的写问题

高速缓存关于读的操作非常简单。

> [!tip] 高速缓存读操作：首先，在高速缓存中查找所需字 $w$ 的副本
> 
> 如果命中，立即返回字 $w$ 给 CPU
> 
> 如果不命中，从存储器层次结构中较低层中取出包含字 $w$ 的块，将这个块存储到某个高速缓存行中，然后返回字 $w$
> + 缓存到高速行时，可能会驱逐某个有效行


写的情况就要复杂一些了。假设我们 **要写一个已经缓存了的字 $w$ (_写命中_)**。在高速缓存更新了它的 $w$ 的副本之后，**怎么更新 $w$ 在层次结构中紧接着低一层中的副本呢？**

> [!question] 写命中后如何更新低层存储期中的副本？
> 
> 有下面几个方法
> + **直写**：立即将 $w$ 的高速缓存块写回到紧接着的低一层中。虽然简单，但是 **每次写都会引起总线流量**
> + **写回**：尽可能的推迟跟新。只有当替换算法驱逐更新后的块时，才把它写入低一层。然而，写回能 **显著地减少总线流量**，但是它的缺点是 **增加了复杂性**
> 

高速缓存必须 **为每个高速缓存行维护一个额外的 _修改位_ 表明这个高速缓存块 _是否被修改过_**

前面我们讨论了写命中是如何处理的，下面还需要解决 **写不命中**

> [!question] 写不命中后如何处理？
> 
> 有下面几种处理方法
> 
> + **写分配**：加载相应的低一层中的块到高速缓存中，然后更新这个高速缓存块。写分配试图利用写的空间局部性，但是缺点是 **每次不命中都会导致一个块从低一层传送到高速缓存**
> + **非写分配**, 避开高速缓存，**直接把这个字写到低一层中**
> 

### 真实的高速写缓存层次结构解剖

实际上，高速缓存既保存数据，也保存指令。**只保存指令的高速缓存称为 i-cache**。**只保存程序数据的高速缓存称为 d-cache**。既保存指令又包括数据的高速缓存称为 **统一的高速缓存**

> [!tip] **现代处理器包括独立的 i-cache 和 d-cache**
> 
>  这样做有很多原因
>  
>  + 有两个独立的高速缓存，CPU 能够 **同时读一个指令字和一个数据字**。**i-cache通常是只读的，因此比较简单**
> 	 + 通常会针对不同的访问模式来优化这两个高速缓存，它们可以有不同的块大小，相联度和容量
> + 使用不同的髙速缓存也确保了 **数据访问不会与指令访问形成冲突不命中**，反过来也是一样，代价就是可能会引起容量不命中增加
> 

下图是 Intel Core i7 处理器的高速缓存层次结构。每个 CPU 芯片有四个核。每个核有自己私有的 L1 i-cache、L1 d-cache 和 L2 统一的高速缓存。所有的核共享片上的 L3 统一的高速缓存。这样的层次结构的特性就是所有的 SRAM 高速缓存存储器都在 CPU 芯片上

![[Pasted image 20241028160116.png]]

### 高速缓存参数的性能影响

有许多指标来衡量高速缓存的性能

| 指标    | 描述                                | 计算       |
| :---- | --------------------------------- | -------- |
| 不命中率  | 在一个程序执行或程序的一部分执行期间，**内存引用不命中的比率** | 不命中数/引用数 |
| 命中率   | 在一个程序执行或程序的一部分执行期间，**内存引用命中的比率**  | 命中数/引用数  |
| 命中时间  | 从高速缓存传送一个字到 CPU 所需的时间             |          |
| 不命中处罚 | 由于不命中所需要的额外的时间                    |          |

> [!tip] 不命中触发
> 
> + L1 不命中需要从 L2 得到服务的处罚，通常是数 $10$ 个周期
> + 从 L3 得到服务的处罚，$50$ 个周期
> + 从主存得到的服务的处罚，$200$ 个周期
> 


## 缓存友好代码

我们介绍了局部性的思想，而且定性地谈了一下什么会具有良好的局部性。明白了高速缓存存储器是如何工作的，我们就能更加准确一些了

> [!tip] 
> **局部性比较好的程序更容易有较低的不命中率，而不命中率较低的程序往往比不命中率较高的程序运行得更快**
> 

因此，从具有良好局部性的意义上来说，好的程序员总是应该试着去编写 **高速缓存友好** 的代码。下面就是我们用来确保代码高速缓存友好的基本方法

+ **让最常见的情况运行得快**。程序通常把大部分时间都花在少量的核心函数上，而这些函数通常把大部分时间都花在了少量循环上。所以要把注意力集中在核心函数里的循环上，而忽略其他部分
+ **尽量减小每个循环内部的缓存不命中数量**。在其他条件（例如加载和存储的总次数）相同的情况下，不命中率较低的循环运行得更快

> [!example] 考虑 `sumvec` 函数。这个函数高速缓存友好吗？
> 
> ```c
> int sumvec(int v[N]) {
>     int i, sum = 0;
>     for(i = 0; i < N; i++) {
>         sum += v[i];
>     }
>     return sum;
> }
> ```
> 
> 首先，注意对于局部变量 `i` 和 `sum`，循环体有良好的时间局部性。实际上，因为它们都是局部变量，任何合理的优化编译器都会把它们缓存在寄存器文件中，也就是存储器层次结构的最高层中
> 
> 现在考虑一下对向量 `v` 的步长为 $1$ 的引用。一般而言，如果一个高速缓存的块大小为 $B$ 字节，那么一个 **步长为 $k$ 的引用模式**（这里 $k$ 是以字为单位的）平均每次循环迭代会有` min(1,(wordsize * k / B)` 次 **缓存不命中**。当是 $k = 1$ 时，它取最小值，所以对 `v` 的步长为 $1$ 的引用确实是高速缓存友好的
> 
> 

例如，假设 `v` 是块对齐的，字为 $4$ 个字节，高速缓存块为 $4$ 个字，而高速缓存初始为空（冷高速缓存）。然后，无论是什么样的高速缓存结构，对 `v` 的引用都会得到下面的命中和不命中模式

![[Pasted image 20241028161051.png]]

在这个例子中，对 `v[0]` 的引用会不命中，而相应的包含 `v[0] ~ v[3]` 的块会被从内存加载到高速缓存中。因此，接下来三个引用都会命中。对 `v[4]` 的引用会导致不命中，而一个新的块被加载到高速缓存中，接下来的三个引用都命中，依此类推。总的来说，四个引用中，三个会命中，在这种冷缓存的情况下，这是我们所能做到的最好的情况了

> [!important] 
> 
> 上述实例说明了 **两关于编写高速缓存友好的代码的重要问题**
> 
> + **对局部变量的反复引用是好的**，因为编译器能够将它们缓存在寄存器文件中（时间局部性）
> + **步长为 $1$ 的引用模式是好的**，因为存储器层次结构中所有层次上的缓存都是将数据存储为连续的块（空间局部性）
> 

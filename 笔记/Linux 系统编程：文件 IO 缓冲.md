# 文件 IO 缓冲

在 [[操作系统：文件系统的实现#效率与性能]] 中，我们知道为了提高 IO 效率，有些系统有一块独立内存用作 **缓冲区缓存**。存储在缓冲区中的 **块将很快再次使用**；其他系统采用 **页面缓存** 来缓存文件数据

> [!tip] 页面缓存
> 
> 采用 [[操作系统：虚拟内存]] 技术，文件数据 **按页面** 缓存，而不是按文件块缓存
> 

Unix/Linux 采用 **页面缓存** 来缓存进程页面和文件数据，这称为 **统一虚拟内存**

## 内核缓冲区高速缓存

`read()` 和 `write()` 系统调用在操作磁盘文件时不会直接发起磁盘访问，而是仅仅在 **用户空间缓冲区** 与 **内核缓冲区高速缓存** 之间复制数据

例如，如下 `write()` 调用将 $3$ 个字节的数据从用户空间缓冲区复制到内核缓冲区后立即返回

```c
write(fd, "abc", 3);  // 复制到内核缓冲区后，立即返回
```

`write()` 返回后的某个时刻，内核会将其缓冲区中的数据写入磁盘

> [!tip]
> 
> 系统调用与磁盘操作并不同步。在此期间，如果另一个进程需要读取该文件中的内容，那么内核将从缓冲区高速缓存中提供这些数据，而不是从文件中读取
> 

对于输入而言，和 `write()` 系统调用类似，**内核首先从磁盘中读取数据并存储到内核缓冲区中**；`read()` 系统调用将从内核缓冲区读取数据，直至版缓冲区中的数据读取完。此时，内核会将文件的下一段内容读入到内核缓冲区中

> [!important] 
> 
> 采用缓冲区，可以使的 `read()` 和 `write()` 调用操作更为快速，因为它们不需要等待磁盘操作。同时，因为减少了内核必须指向磁盘 IO 的次数，所以这样设计是极为高效的
> 

Linux 内核对内核缓冲区高速缓存的大小没有固定上限，**内核会分配尽可能多的缓冲区高速缓存页**。然而，受限于两个因素：**可用物理内存容量** 和 **非缓冲目的的物理内存需求**

> [!tip] 
> 
> 若可用内存不足，内核会将修改过的缓冲区高速缓存页内容刷新到磁盘，并释放其供系统重用
> 

## 控制文件 I/O 的内核缓冲

当操作磁盘文件时，缓冲大块数据以减少系统调用，C 语言 `<stdio.h>` 头中的库函数正是这么做的。因此，使用 `stdio` 库可以使编程者免于自行处理对数据的缓冲，无论是调用 `write()` 来输出，还是调用 `read()` 来输入。这部分内容在 [[C 语言：标准 IO 流#文件缓冲]] 中，已经介绍过了

**强制刷新内核缓冲区到输出文件是可能的**。这有时很有必要，例如，当应用程序要确保在继续操作前将输出 **真正 _写入磁盘_** 或者 **至少写入 _磁盘的硬件高速缓存_** 中

在学习用于控制内核缓冲的系统调用之前，有必要熟悉两个定义：**同步 IO _数据完整性_** 和 **同步 IO _文件完整性_**

### synchronized IO completion

SUSv3 将 **synchronized IO completion** 定义为：某一 IO 操作，要么已成功完成到磁盘的数据传递，要么被诊断为不成功

SUSv3 标准定义两种 synchronized I/O completion：**同步 IO _数据完整性_** 和 **同步 IO _文件完整性_**。它们之间的区别只涉及用于描述 **文件的元数据**

> [!tip] 文件元数据
> 
> 亦即内核针对文件而存储的数据
> 
> 针对文件元数据，在 [[操作系统：文件系统#文件属性]] 已经简单介绍过了，后续在 [[Linux 系统编程：文件属性]] 中会相信介绍
> 

SUSv3 定义的第一种 synchronized I/O completion 类型是 **Synchronized I/O data integrity completion**：确保针对文件的一次更新传递了 **_足够_** 的信息到磁盘，以便于之后对数据的获取

> [!tip] 读操作：任何影响到请求数据的的挂起写操作，都会在读操作之前执行
> 
> **就读操作而言**，这意味着 **被请求的文件数据已经从磁盘传递给进程**。若存在任何影响到所请求数据的挂起写操作，那么在执行读操作之前，会将这些数据传递到磁盘
> 

> [!tip] 写操作：写请求指定的数据和用于获取数据的文件元数据，都已传递至磁盘完毕
> 
> **就写操作而言**，这意味着 **写请求所指定的数据** 已传递至磁盘完毕，且 **用于获取数据的文件元数据** 也已传递至磁盘完毕
> 
> 注意：用于获取数据的文件元数据并非是所有元数据。也就是说，synchronized I/O data integrity completion 保证的是最基本的修改已经传递到磁盘
> 

SUSv3 定义的另一种 synchronized I/O completion 类型是 **Synchronized I/O file integrity completion**：确保针对文件的一次更新传递了 **_所有_** 的信息到磁盘，也就是说，**所有发生更新的文件元数据都传递到磁盘上**，即使有些在后续对文件数据的读操作中并不需要

### 系统调用


`fsync()` 系统调用将使 **缓冲数据** 和与打开文件描述符 `fd` 相关的 **所有元数据都刷新到磁盘上**。调用 `fsync()` 会强制使文件处于 Synchronized IO file integrity completion 状态

```c
#include <unistd.h>

int fsync(int fd);
/* 成功返回 0； 失败返回 -1*/
```

> [!tip] `fsync()` 系统调用：强制文件处于 **Synchronized IO file integrity completion**
> 
> 强制文件处于 **同步 IO 文件完整性完成**：仅在对磁盘设备或者至少是其高速缓存的传递完成后，`fsync()` 调用才会返回
> 

`fdatasync()` 系统调用的运作类似于 `fsync()`，只是强制文件处于 **Synchronized IO data  integrity completion** 的状态

```c
#include <unistd.h>

int fdatasync(int fd);
/* 成功返回 0； 失败返回 -1*/
```

> [!tip] `fdatasync()` 系统调用：强制文件处于  **Synchronized IO data  integrity completion**
> 
> `fdatasync()` 可能会减少对磁盘操作的次数，由 `fsync()` 调用请求的两次变为一次
> 
> 若修改了文件数据，而文件大小不变，那么调用 `fdatasync()` 只强制进行了数据更新；诸如最近修改时间戳之类的元数据属性发生了变化，那么是无需传递到磁盘的
> 

> [!attention] 
> 
> Linux 2.2 以及更早版本的内核将 `fdatasync()` 实现为对 `fsync()` 的调用，因而性能也未获得提升
> 

`sync()` 系统调用会使包含更新文件信息的 **所有内核缓冲区**刷新到磁盘上

```c
#include <unistd.h>

void sync(void);
```

> [!tip] `sync()` 系统调用：文件更新的所有信息都保存在磁盘上
> 
> 内核缓冲区中的所有信息，包括 数据块、指针块、元数据等
> 
> 在 Linux 实现中，`sync()` 调用 **仅在所有数据已传递到磁盘上或者至少是磁盘的高速缓存时返回**
> 

### flags 标志

调用 `open()` 函数可以指定 $3$ 个标志用于控制 synchronized IO completion

| `flags`   | 描述                                          |
| :-------- | :------------------------------------------ |
| `O_SYNC`  | 类似于 `fsync()`；要求写操作保证 **同步 IO 文件完整性完成**     |
| `O_DSYNC` | 类似于 `fdatasync()`；要求写操作保证 **同步 IO 数据完整性完成** |
| `O_RSYNC` | 必须配合上面两个标志之一使用。将其结合到读操作中                    |

> [!tip] 
> 
> `O_RSYNC | O_DSYNC`：在读操作之前，像执行 `O_DSYNC` 标志那样完成所有待处理的写操作
> 
> `O_RSYNC | O_SYNC`：在读操作之前，像执行 `O_SYNC` 标志那样完成所有待处理的写操作
> 

> [!attention] 
> 
> 2.6.33 版本之前的 Linux 内核并未实现 `O_DSYNC` 和 `O_RSYNC` 标志。glibc 头文件当时只是将这些常量定义为 `O_SYNC` 标志
> 
> 始于 2.6.33 版本，Linux 内核实现了 `O_DSYNC` 标志的功能
> 

### 缓冲刷新线程

若内容发生变化的内核缓冲区在 30 秒内未经显式方式同步到磁盘上，则一条长期运行的内核线程会确保将其刷新到磁盘上

 为了 **规避缓冲区与相关磁盘文件内容长期处于不一致状态**，以至于在系统崩溃时发生数据丢失的问题
+ 在 Linux 2.6 版本中，该任务由 `pdflush` 内核线程执行
+ 在 Linux 2.4 版本中，则由 `kupdated` 内核线程执行

文件 `/proc/sys/vm/dirty_expire_centisecs `规定了在 `pdflush` 刷新之前脏缓冲区必须达到的“年龄”（以 `1%` 秒为单位）。位于同一目录下的其他文件则控制了 `pdflush` 操作的其他方面

## IO 缓冲小结

下图概括了 `<stdio.h>` 库函数和内核采用的缓冲，以及对各种缓冲类型的控制机制。首先是通过 `stdio` 库将用户数据传递到 `stdio` 缓冲区，该缓冲区位于用户态内存区。当缓冲区填满时，`stdio` 库会调用 `write()` 系统调用，将数据传递到内核高速缓冲区（位于内核态内存区）。最终，内核发起磁盘操作，将数据传递到磁盘

![Pasted image 20241018220409|600](http://cdn.jsdelivr.net/gh/duyupeng36/images@master/obsidian/1755705681182-11c7b44d4c2f433aa96a3e93847cd6ef.png)

左侧所示为可于任何时刻显式强制刷新各类缓冲区的调用；右侧所示为**促使刷新自动化的调用**：一是通过禁用 `stdio` 库的缓冲，二是在文件输出类的系统调用中启用同步，从而使每个 `write()` 调用立刻刷新到磁盘

## 直接 IO

始于内核 2.4，**Linux 允许应用程序在执行磁盘 IO 时 _绕过缓冲区高速缓存_**，从用户空间直接将数据传递到文件或磁盘设备。有时也称此为 **直接 IO** 或者 **裸 IO**

> [!attention] 
> 
> 这是 Linux 特有，SUSv3 并未对其进行规范
> 

有时会将直接 IO 误认为获取快速 IO 性能的一种手段。然而，对于大多数应用而言， **使用直接 IO 可能会大大降低性能**。这是因为为了提高 IO 性能，内核针对缓冲区高速缓存做了不少优化，其中包括：按顺序预读取，在成簇磁盘块上执行 IO，允许访问同一文件的多个进程共享高速缓存的缓冲区

> [!important] 
> 
> 为了执行直接IO，在调用 `open()` 函数时应该指定 `O_DIRECT` 标志，该标志从内核 2.4.10 开始有效
> 

## 混合使用库函数和系统调用进行文件 IO

在同一文件上执行 I/O 操作时，还可以将 **系统调用** 和 **标准 C 语言库函数** 混合使用。`fileno()` 和 `fdopen()` 函数有助于完成这一工作

```c
#include <stdio.h>

int fileno(FILE * stream);
/* 返回 FILE* 对应的文件描述符，错误返回 -1 */

FILE * fdopen(int fd, char *mode);
/* 返回一个文件指针，失败返回 NULL*/
```

> [!tip] `fileno()` 函数将返回相应的文件描述符
> 

> [!tip] `fdopen()` 函数与 `fileno()` 函数的功能相反
> 
> 给定一个文件描述符，该函数将创建了一个使用该描述符进行文件 IO 的相应流
> 
> `mode` 参数参考 [[C 语言：标准 IO 流#打开文件#模式]]
